{
    "collab_server" : "",
    "contents" : "### Note:\n\n# When scrape Chinese, encoding mush be executed\n# Sys.setlocale(category = \"LC_CTYPE\", local = \"Chinese\")\n\n\n\n\n#########################################################################\n### using XML package\n# scrapping tables\n\nlibrary(XML)\nsrts<-htmlParse(\"http://www.cpppc.org:8082/efmisweb/ppp/projectLivrary/getProjInfo.do?projId=45f747ac429b4773a88938efbde7c197\")\nclass(srts)\n\nsrts.table<- readHTMLTable(srts,stringsAsFactors = FALSE)\n\n\n## Trial\n\na <- htmlParse(\"http://www.cpppc.org:8082/efmisweb/ppp/projectLivrary/getProjInfo.do?projId=cb545ec0888746bbaa31b71926cb56fc\")\n# for this website, there is no title, so it needs a little transition\ntable <- readHTMLTable(a, stringsAsFactors = FALSE)[[1]]\n\n########################################################################\n### using different tools\n\n### use httr to send post request\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(rvest)\n# functions\ngetPPPList_unit <- function(url, page, proxy = NULL){\n  \n  res <- POST(url,\n     encode=\"form\",\n     body=list(queryPage=page,\n               distStr=\"\",\n               induStr=\"\",\n               investStr=\"\",\n               projName=\"\",\n               sortby=\"\",\n               orderby=\"\",\n               stageArr=\"\"), use_proxy(proxy[1, 1], proxy[1, 2]), timeout(15))\n  resC <- content(res, as = 'text', encoding = 'utf-8')\n  # But RJSONIO only gives list not dataframe\n  # jsonlite will have encoding problems here, need to change sys encoding\n\n  info <- jsonlite::fromJSON(resC, flatten = TRUE)$list \n  return(info)\n} \n\n\n# to get free proxy from internet\ngetProxy <- function() {\n  # get free proxy from http://www.free-proxy-list.net/\n  url <- 'http://www.free-proxy-list.net/'   #xpath = //*[@id=\"proxylisttable\"]\n  # url <- 'http://www.kuaidaili.com/proxylist/3/'\n  table <- url %>%\n    read_html() %>%\n#    html_nodes(xpath = '//*[@id=\"index_free_list\"]/table/tbody') %>%\n    html_table()\n  \n  return(table[[1]])  \n}\n\n# start scraping\ngetPPPList <- function(url, endPage, startPage = 1) {\n  # get proxy from special website, and every time scrapes, it will have 300 proxies, so\n  # the limit of the random number is 300. Need to check this every some time\n  page <- startPage # set up initial value\n  times <- 0\n  \n  # first generate proxy for scapring\n  proxyPool <- getProxy()[,1:2]\n  startTime <- Sys.time() # Get the start time, if it exceeds 1 hour, load proxy again.\n  \n  \n  \n  # Since for this case we got 301 proxies, so map the proxy table\n  proxyIndex <- 1 #proxyIndex starts from1\n  \n  repeat {\n    \n    PPPList <- tryCatch({\n      \n      getPPPList_unit(url = url, page, proxy = proxyPool[proxyIndex,])\n      \n    },error = function(cond) {\n      message(paste('\\n', Sys.time(), \" Proxy doestn't work or ...\\n\"))          \n      message(cond)\n      return(1)\n    })\n    \n    if (length(PPPList) == 1 $ PPPList == 1) {      times <- 0\n      proxyIndex <- proxyIndex + 1# if proxy does't work or 30 pages are scraped, change proxy\n      \n      if (proxyIndex == 301) {\n        message('\\nrefreshe proxy pool...')\n        proxyPool <- getProxy()[,1:2]\n        proxyIndex <- 1\n      } \n      } else {\n   \n      \n      if (times == 0) {\n        totalPPPList <- PPPList\n      } else {\n        # bind the new list to the total list\n        totalPPPList <- rbind(totalPPPList, PPPList)\n      }\n      \n      times <- times + 1\n      page <- page + 1\n      # here one more control, when page reaches 499, change proxy\n      # But if using proxy, usually no need to worry about the ip block\n      if (times == 1000000) {\n        # Just in case, usually it will not reach that many times\n        stop(\"1000000 times, change proxy...\\n\")\n        # randomNum <- round(runif(1, 1, 300))\n      } else {\n        message(paste(\"page\", page - 1))\n      }\n    }\n    if (page > endPage) break\n    endTime <- Sys.time()\n    if (is.integer((endTime - startTime)/5400)) {\n      message(\"\\nRefresh proxy pool...\")\n      proxyPool <- getProxy()[,1:2]\n    }\n  }\n  \n  return(totalPPPList)\n}\n\n######################################################################\n\n# this url is got by analysing the original url <- 'http://www.cpppc.org:8082/efmisweb/ppp/projectLivrary/toPPPList.do'\n# by the tools provided by the explorer, under network part, each request can be simulated\nurl <- 'http://www.cpppc.org:8082/efmisweb/ppp/projectLivrary/getPPPList.do?tokenid=null'\nendPage <- 1309\n\ninfo <- getPPPList(url, endPage)\n\nSys.setlocale(category = \"LC_CTYPE\", local = \"Chinese\")\n\n## Do some cleaning about the data, since we don't need all the columns\ninfoIndex1 <- c(1, 3, 6, 12, 18, 20, 22, 23, 25, 28, 30, 32, 33, 35, 39, 52, 53, 57, 69)\ninfoIndex <- c(\"PROJ_NAME\", \"PROJ_SURVEY\",  \"PRV\", \"INVEST_COUNT\", \"PROJ_STATE_NAME\", \n               \"IVALUE\", \"IVALUE2\", \"START_UNAME\", \"PRV1\", \"PRV2\", \"ESTIMATE_COPER\", \"RETURN_MODE_NAME\",\n               \"PROJ_TYPE_NAME\", \"START_TIME\", \"CREATING_UNAME\", \"OPERATE_MODE_NAME\",\n               \"CODE_NAME\", \"DIST_CODE\", \"DATABASE_NAME\", \"CREATING_TIME\" )\ninfo1 <- info[,infoIndex]\n\n\n# Since there is a \"\\r\\n\" and \"\\t\" in some strings, so this needs to be removed\n# probably write a function about this\ninfo1[, 2] <- sapply(info1[, 2], function(x) gsub(\"\\r\\n\", \"\", x))\ninfo1[, 2] <- sapply(info1[, 2], function(x) gsub(\"\\r\", \"\", x))\ninfo1[, 2] <- sapply(info1[, 2], function(x) gsub(\"\\t\", \"\", x))\ninfo1[, 2] <- sapply(info1[, 2], function(x) gsub('\\\"', \"\", x))\n\n\n\nwrite.table(info1, file = 'C:\\\\Users\\\\User\\\\Desktop\\\\1\\\\111.txt', fileEncoding = 'UTF-8', sep = '?')\n\n# write.csv(info1[,], file = 'C:\\\\Users\\\\User\\\\Desktop\\\\1\\\\111.csv', fileEncoding = 'UTF-8')\n# when write to file utf-8 encoding is needed\n\n\nsiftWater <- function(array, lang = 'CN') {\n  Sys.setlocale(category = \"LC_CTYPE\", local = \"Chinese\")\n  if (lang == 'CN') {\n    keywords1 <- c('污水', '雨水', '海绵城市', '管网', '管廊', '河道', '水环境',\n                '水域', '流域', '水系', '湖泊', '中水', '供水', '排水', '水利',\n                '南水北调', '水土', '港口', '水治理', '自来水', '水厂', '水塘',\n                '再生水', '水工', '引水', '配水', '地表水', '地下水', '灌溉', '给水',\n                '水权', '节水', '补水', '饮用水', '水源', '水库', '水电', '雨污分离',\n                '湿地', '水体', '调水', '防洪', '输水', '河治理', '河综合治理',\n                '河改造', '水生态', '河综合整治', '水渠', '海水', '淡化', '流域综合管理',\n                '水资源', '调蓄', '水污染', '水务', '流域治理', '水质', '水塘', '雨污分流',\n                '净水', '废水', '水沟')\n  }\n  # better put fisrt level and 2nd level keyword to sift\n  \n  keywords <- paste(keywords, collapse = '|')\n  index <- grep(keywords, array)\n  return(index)\n  \n}\n\nwater1 <- siftWater(info1[, 'PROJ_NAME'])\nwater2 <- siftWater(info1[, 'PROJ_SURVEY'])\nwater <- union(water1, water2)\ninfo1_water <- info1[water,]\n\nwrite.table(info1_water, file = 'C:\\\\Users\\\\User\\\\Desktop\\\\1\\\\111.txt', fileEncoding = 'UTF-8', sep = '?')\n\n######################################################################################\n\nurl <- 'http://www.iea.org/statistics/statisticssearch/report/?year=2013&country=MYANMAR&product=Coal'\ntable <- url %>%\n  read_html() %>%\n  html_nodes(xpath = '//*[@id=\"stats-container\"]/div[2]/table') %>%\n  html_table()[[1]]\n\n\nres <- POST(url,\n            encode=\"form\",\n            body=list(country=\"JORDAN\",\n                      product=\"coal\",\n                      year = 2013), use_proxy(proxy[1, 1], proxy[1, 2]))\nresC <- content(res, as = 'text', encoding = 'utf-8')\n# But RJSONIO only gives list not dataframe\n# jsonlite will have encoding problems here, need to change sys encoding\n\ninfo <- jsonlite::fromJSON(resC, flatten = TRUE) \nreturn(info)\n\n\n\n###############################################################################\n# Machine Learning to categorize projects\n\n\n\n############################################################################\n# CSR website scraping\n# Since this is not a json string, but a javascript object, without quotes,\n# So package V8 is needed to parse\nlibrary(V8)\nlibrary(stringi)\n\nengine <- v8()\n\n\nurl <- 'http://stockdata.stock.hexun.com/zrbg/data/zrbList.aspx?'\ndate <- '2015-12-31'\npage <- 1\n\ndoCSRCorp <- function(startPage, endPage) {\n  page <- startPage\n  engine <- v8()\n  \n  \n  repeat {\n  res <- GET(url, query = list(date = date,\n                               count = 20,\n                               pname = 20,\n                               titType = 'null',\n                               page = page), use_proxy(proxy[1, 1], proxy[1, 2]), timeout(15))\n  \n  resC <- content(res) %>% \n    stri_replace_first_fixed(\"hxbase_json1(\", \"var dat=\") %>% \n    stri_replace_last_fixed(\")\", \"\") %>% \n    engine$eval()\n  \n  \n  resC <- engine$get('dat')$list\n  \n  if (page == 1) {\n    totalList <- resC\n  } else {\n    # bind the new list to the total list\n    totalList <- rbind(totalList, resC)\n  }\n  \n  page <- page + 1\n  \n  if (page > endPage) break\n  \n}\n\n  return(totalList)\n}\n\nx <- doCSRCorp(1,151)\n\n\n\nresC <- content(res) %>% \n  stri_replace_first_fixed(\"hxbase_json1(\", \"var dat=\") %>% \n  stri_replace_last_fixed(\")\", \"\") %>% \n  engine$eval()\n\n\nresC <- engine$get('dat')\n\n# This failed, but put it here just in case\n# resC <- gsub('hxbase_json1', '', resC)\n# resC <- gsub('\\\\(\\\\{', '\\\\{', resC)\n# resC <- gsub('\\\\}\\\\)', '\\\\}', resC)\n# resC <- gsub('sum:', '\"sum\":', resC)\n# resC <- gsub('list', '\"list\"', resC)\n# resC <- gsub('Number', '\"Number\"', resC)\n# resC <- gsub('StockNameLink', '\"StockNameLink\"', resC)\n# resC <- gsub('industry', '\"industry\"', resC)\n# resC <- gsub('stockNumber', '\"stockNumber\"', resC)\n# resC <- gsub('industryrate', '\"industryrate\"', resC)\n# resC <- gsub('Pricelimit', '\"Pricelimit\"', resC)\n# resC <- gsub('lootingchips', '\"lootingchips\"', resC)\n# resC <- gsub('Scramble', '\"Scramble\"', resC)\n# resC <- gsub('rscramble', '\"rscramble\"', resC)\n# resC <- gsub('Strongstock', '\"Strongstock\"', resC)\n# resC <- gsub('Hstock', '\"Hstock\"', resC)\n# resC <- gsub('Wstock', '\"Wstock\"', resC)\n# resC <- gsub('target', '\"target\"', resC)\n# resC <- gsub('Tstock', '\"Tstock\"', resC)\n\n\nresC1 <- jsonlite::fromJSON(resC) \n\n\n############################################################################\n# Yellow river quality data\n\ngetWaterQ_MEP_all_unit <- function(year, week, station1, station2, proxy = NULL) {\n  res <- GET(url,\n             query = list(year = year,\n                          wissue = week), use_proxy(proxy[1, 1], proxy[1, 2]))\n  resC <- content(res, as = 'text', encoding = 'utf-8')%>%\n    read_html()%>%\n    html_table(fill = TRUE)\n  \n  table <- resC[[1]]\n  startIndex <- which(table[,1] == station1)\n  endIndex <- which(table[,1] == station2)\n  \n  table <- table[startIndex:endIndex, 1:14]\n  return(table)\n}\n\n\ngetWaterQ_MEP_all <- function(year, week, station1, station2){\n  message('Since the number of monitoring stations changes with time, so make sure in your\n        scraping period, the number of monitoring stations is consistent.')\n  \n  url <- 'http://datacenter.mep.gov.cn/report/water/water.jsp?year=2016&wissue=45&x=29&y=6'\n  if (length(year) != 1) message('Caution!!! the result can be wrong if you input more than 1 year, since the number\n                                 of stations change with time.')\n  times <- 0\n  startTime <- Sys.time() # Get the start time, if it exceeds 1 hour, load proxy again.\n  \n  \n  \n  # deal with proxy\n  proxyIndex <- 1 \n  proxyPool <- getProxy()[,1:2]\n  page <- min(week)\n  \n  repeat {\n    \n    table <- tryCatch({\n      \n      getWaterQ_MEP_all_unit(year = year, week = page, station1 = station1, \n                                  station2 = station2, proxy = proxyPool[proxyIndex, ])\n    },error = function(cond) {\n      message(paste('\\n', Sys.time(), \" Proxy doestn't work or ...\\n\"))          \n      message(cond)\n      return(1)\n    })\n    \n    if (length(table) == 1 & table == 1) {\n      times <- 0\n      proxyIndex <- proxyIndex + 1\n      \n      if (proxyIndex == 300) { # there are 300 proxies at a time, if reaches 300, need to refresh\n        message('\\nrefreshe proxy pool...')\n        proxyPool <- getProxy()[,1:2]\n        proxyIndex <- 1\n      }\n\n    } else {\n      \n      if (times == 0) {\n        totalTable <- table\n      } else {\n        # bind the new list to the total list\n        totalTable <- rbind(totalTable, table)\n      }\n      \n      times <- times + 1\n      page <- page + 1\n      # here one more control, when page reaches 499, change proxy\n      # But if using proxy, usually no need to worry about the ip block\n      if (times == 1000000) {\n        # Just in case usually it will not reach this time\n        stop(\"1000000 times, change proxy...\\n\")\n        # randomNum <- round(runif(1, 1, 300))\n      } else {\n        message(paste(\"\\nweek\", page - 1))\n      }\n      \n    }\n    if (page > max(week)) break\n    endTime <- Sys.time()\n    if (is.integer((endTime - startTime)/5400)) {\n      message(\"\\nRefresh proxy pool...\")\n      proxyPool <- getProxy()[,1:2]\n    }\n  }\n  \n  return(totalTable)\n}\n\n\n\n\nSys.setlocale(category = \"LC_CTYPE\", local = \"Chinese\")\n\n\nwrite.table(a, file = 'C:\\\\Users\\\\User\\\\Desktop\\\\1\\\\111.txt')\n\n  \n\n\n\na <- getWaterQ_MEP_all(2016,2:5, 1, 148)\n# for this website, there is no title, so it needs a little transition\ntable <- readHTMLTable(a, stringsAsFactors = FALSE)[[1]]\n\n\n\n#################\n\n",
    "created" : 1481135172305.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "317|82|331|0|\n334|62|404|0|\n",
    "hash" : "2756518180",
    "id" : "8C9E4550",
    "lastKnownWriteTime" : 1480399676,
    "last_content_update" : 1481265643049,
    "path" : "C:/Users/user/Google Drive/CWR/R/scraping.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}