{
    "collab_server" : "",
    "contents" : "\n\n# Get water quality data\n\n\n#' get PPP list from a single page\n#' @param url the url of CSR Rating, usually the default website\n#'\n#' @param year In which year you would like to scrape\n#' @param week In which week you would like to scrape\n#' @param station1 the start station index on the page\n#' @param station2 the end station index on the page\n#' @param proxy if you wnat to use a proxy to avoid blocking, you can input a proxy, otherwise leave\n#' it blank.\n#' @return A table of PPP projects collected from your input page\n#' @importFrom httr GET timeout content\n#' @importFrom rvest html_table %>%\n#' @importFrom xml2 read_html\n#' @export\n# @examples\n# add(1, 1)\n# add(10, 1)\ngetWaterQ_MEP_all_unit <- function(url, year, week, station1, station2, proxy = NULL) {\n  res <- GET(url,\n             query = list(year = year,\n                          wissue = week), use_proxy(proxy[1, 1], proxy[1, 2]))\n  resC <- content(res, as = 'text', encoding = 'utf-8')%>%\n    read_html()%>%\n    html_table(fill = TRUE)\n\n  table <- resC[[1]]\n  startIndex <- which(table[,1] == station1)\n  endIndex <- which(table[,1] == station2)\n\n  table <- table[startIndex:endIndex, 1:14]\n  return(table)\n}\n\n\n\n\n\n\n#' get PPP list from a single page\n#'\n#' @param year In which year you would like to scrape\n#' @param week In which week you would like to scrape\n#' @param station1 the start station index on the page\n#' @param station2 the end station index on the page\n#' @details\n#' Get monitoring data of different stations from Minitsry of Environmental Protection of China. Using this function\n#' you will get data of all the stations. Since the number of stations vary with time, using this function, you have\n#' to make sure that within the period you are scrapping, the number of stations keep consistant.\n#' @export\n#' @examples\n#'\n#' \\dontrun{\n#' # get data from station1 to station5 of the 3rd week of 2016\n#' getWaterQ_MEP_all(2016, 3, 1, 5)\n#' }\n#'\n\ngetWaterQ_MEP_all <- function(year, week, station1, station2){\n  message('Since the number of monitoring stations changes with time, so make sure in your\n          scraping period, the number of monitoring stations is consistent.')\n\n  url <- 'http://datacenter.mep.gov.cn/report/water/water.jsp?year=2016&wissue=45&x=29&y=6'\n  if (length(year) != 1) message('Caution!!! the result can be wrong if you input more than 1 year, since the number\n                                 of stations change with time.')\n  times <- 0\n  startTime <- Sys.time() # Get the start time, if it exceeds 1 hour, load proxy again.\n\n\n\n  # deal with proxy\n  proxyIndex <- 1\n  proxyPool <- getProxy()[,1:2]\n  page <- min(week)\n\n  repeat {\n\n    table <- tryCatch({\n\n      getWaterQ_MEP_all_unit(url, year = year, week = page, station1 = station1,\n                             station2 = station2, proxy = proxyPool[proxyIndex, ])\n    },error = function(cond) {\n      message(paste('\\n', Sys.time(), \" Proxy doestn't work or ...\\n\"))\n      message(cond)\n      return(1)\n    })\n\n    if (length(table) == 1) {\n      times <- 0\n      proxyIndex <- proxyIndex + 1\n\n      if (proxyIndex == 300) { # there are 300 proxies at a time, if reaches 300, need to refresh\n        message('\\nrefreshe proxy pool...')\n        proxyPool <- getProxy()[,1:2]\n        proxyIndex <- 1\n      }\n\n    } else {\n\n      if (times == 0) {\n        totalTable <- table\n      } else {\n        # bind the new list to the total list\n        totalTable <- rbind(totalTable, table)\n      }\n\n      times <- times + 1\n      page <- page + 1\n      # here one more control, when page reaches 499, change proxy\n      # But if using proxy, usually no need to worry about the ip block\n      if (times == 1000000) {\n        # Just in case usually it will not reach this time\n        stop(\"1000000 times, change proxy...\\n\")\n        # randomNum <- round(runif(1, 1, 300))\n      } else {\n        message(paste(\"\\nweek\", page - 1))\n      }\n\n    }\n    if (page > max(week)) break\n    endTime <- Sys.time()\n    if (is.integer((endTime - startTime)/5400)) {\n      message(\"\\nRefresh proxy pool...\")\n      proxyPool <- getProxy()[,1:2]\n    }\n  }\n\n  return(totalTable)\n}\n",
    "created" : 1481944362246.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2204292359",
    "id" : "732A973D",
    "lastKnownWriteTime" : 1481948900,
    "last_content_update" : 1481948900822,
    "path" : "~/GitHub/gfer/R/getWQ.R",
    "project_path" : "R/getWQ.R",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}