{
    "collab_server" : "",
    "contents" : "### Note:\n\n# When scrape Chinese, encoding mush be executed\n# Sys.setlocale(category = \"LC_CTYPE\", local = \"Chinese\")\n\n\n\n\n#########################################################################\n### using XML package\n# scrapping tables\n\nlibrary(XML)\nsrts<-htmlParse(\"http://www.cpppc.org:8082/efmisweb/ppp/projectLivrary/getProjInfo.do?projId=45f747ac429b4773a88938efbde7c197\")\nclass(srts)\n\nsrts.table<- readHTMLTable(srts,stringsAsFactors = FALSE)\n\n\n## Trial\n\na <- htmlParse(\"http://www.cpppc.org:8082/efmisweb/ppp/projectLivrary/getProjInfo.do?projId=cb545ec0888746bbaa31b71926cb56fc\")\n# for this website, there is no title, so it needs a little transition\ntable <- readHTMLTable(a, stringsAsFactors = FALSE)[[1]]\n\n########################################################################\n### using different tools\n\n### use httr to send post request\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(rvest)\n# functions\ngetPPPList_unit <- function(url, page, proxy = NULL){\n\n  res <- POST(url,\n     encode=\"form\",\n     body=list(queryPage=page,\n               distStr=\"\",\n               induStr=\"\",\n               investStr=\"\",\n               projName=\"\",\n               sortby=\"\",\n               orderby=\"\",\n               stageArr=\"\"), use_proxy(proxy[1, 1], proxy[1, 2]), timeout(15))\n  resC <- content(res, as = 'text', encoding = 'utf-8')\n  # But RJSONIO only gives list not dataframe\n  # jsonlite will have encoding problems here, need to change sys encoding\n\n  info <- jsonlite::fromJSON(resC, flatten = TRUE)$list\n  return(info)\n}\n\n\n# to get free proxy from internet\ngetProxy <- function() {\n  # get free proxy from http://www.free-proxy-list.net/\n  url <- 'http://www.free-proxy-list.net/'   #xpath = //*[@id=\"proxylisttable\"]\n  # url <- 'http://www.kuaidaili.com/proxylist/3/'\n  table <- url %>%\n    read_html() %>%\n#    html_nodes(xpath = '//*[@id=\"index_free_list\"]/table/tbody') %>%\n    html_table()\n\n  return(table[[1]])\n}\n\n# start scraping\ngetPPPList <- function(url, endPage, startPage = 1) {\n  # get proxy from special website, and every time scrapes, it will have 300 proxies, so\n  # the limit of the random number is 300. Need to check this every some time\n  page <- startPage # set up initial value\n  times <- 0\n\n  # first generate proxy for scapring\n  proxyPool <- getProxy()[,1:2]\n  startTime <- Sys.time() # Get the start time, if it exceeds 1 hour, load proxy again.\n\n\n\n  # Since for this case we got 301 proxies, so map the proxy table\n  proxyIndex <- 1 #proxyIndex starts from1\n\n  repeat {\n\n    PPPList <- tryCatch({\n\n      getPPPList_unit(url = url, page, proxy = proxyPool[proxyIndex,])\n\n    },error = function(cond) {\n      message(paste('\\n', Sys.time(), \" Proxy doestn't work or ...\\n\"))\n      message(cond)\n      return(1)\n    })\n\n    if (length(PPPList) == 1 $ PPPList == 1) {      times <- 0\n      proxyIndex <- proxyIndex + 1# if proxy does't work or 30 pages are scraped, change proxy\n\n      if (proxyIndex == 301) {\n        message('\\nrefreshe proxy pool...')\n        proxyPool <- getProxy()[,1:2]\n        proxyIndex <- 1\n      }\n      } else {\n\n\n      if (times == 0) {\n        totalPPPList <- PPPList\n      } else {\n        # bind the new list to the total list\n        totalPPPList <- rbind(totalPPPList, PPPList)\n      }\n\n      times <- times + 1\n      page <- page + 1\n      # here one more control, when page reaches 499, change proxy\n      # But if using proxy, usually no need to worry about the ip block\n      if (times == 1000000) {\n        # Just in case, usually it will not reach that many times\n        stop(\"1000000 times, change proxy...\\n\")\n        # randomNum <- round(runif(1, 1, 300))\n      } else {\n        message(paste(\"page\", page - 1))\n      }\n    }\n    if (page > endPage) break\n    endTime <- Sys.time()\n    if (is.integer((endTime - startTime)/5400)) {\n      message(\"\\nRefresh proxy pool...\")\n      proxyPool <- getProxy()[,1:2]\n    }\n  }\n\n  return(totalPPPList)\n}\n\n######################################################################\n\n# this url is got by analysing the original url <- 'http://www.cpppc.org:8082/efmisweb/ppp/projectLivrary/toPPPList.do'\n# by the tools provided by the explorer, under network part, each request can be simulated\nurl <- 'http://www.cpppc.org:8082/efmisweb/ppp/projectLivrary/getPPPList.do?tokenid=null'\nendPage <- 1309\n\ninfo <- getPPPList(url, endPage)\n\nSys.setlocale(category = \"LC_CTYPE\", local = \"Chinese\")\n\n## Do some cleaning about the data, since we don't need all the columns\ninfoIndex1 <- c(1, 3, 6, 12, 18, 20, 22, 23, 25, 28, 30, 32, 33, 35, 39, 52, 53, 57, 69)\ninfoIndex <- c(\"PROJ_NAME\", \"PROJ_SURVEY\",  \"PRV\", \"INVEST_COUNT\", \"PROJ_STATE_NAME\",\n               \"IVALUE\", \"IVALUE2\", \"START_UNAME\", \"PRV1\", \"PRV2\", \"ESTIMATE_COPER\", \"RETURN_MODE_NAME\",\n               \"PROJ_TYPE_NAME\", \"START_TIME\", \"CREATING_UNAME\", \"OPERATE_MODE_NAME\",\n               \"CODE_NAME\", \"DIST_CODE\", \"DATABASE_NAME\", \"CREATING_TIME\" )\ninfo1 <- info[,infoIndex]\n\n\n# Since there is a \"\\r\\n\" and \"\\t\" in some strings, so this needs to be removed\n# probably write a function about this\ninfo1[, 2] <- sapply(info1[, 2], function(x) gsub(\"\\r\\n\", \"\", x))\ninfo1[, 2] <- sapply(info1[, 2], function(x) gsub(\"\\r\", \"\", x))\ninfo1[, 2] <- sapply(info1[, 2], function(x) gsub(\"\\t\", \"\", x))\ninfo1[, 2] <- sapply(info1[, 2], function(x) gsub('\\\"', \"\", x))\n\n\n\nwrite.table(info1, file = 'C:\\\\Users\\\\User\\\\Desktop\\\\1\\\\111.txt', fileEncoding = 'UTF-8', sep = '?')\n\n# write.csv(info1[,], file = 'C:\\\\Users\\\\User\\\\Desktop\\\\1\\\\111.csv', fileEncoding = 'UTF-8')\n# when write to file utf-8 encoding is needed\n\n\nsiftWater <- function(array, lang = 'CN') {\n  Sys.setlocale(category = \"LC_CTYPE\", local = \"Chinese\")\n  if (lang == 'CN') {\n    keywords1 <- c('污水', '雨水', '海绵城市', '管网', '管廊', '河道', '水环???',\n                '水域', '流域', '水系', '湖泊', '中水', '供水', '排水', '水利',\n                '南水北调', '水土', '港口', '水治???', '自来???', '水厂', '水塘',\n                '再生???', '水工', '引水', '配水', '地表???', '地下???', '灌溉', '给水',\n                '水权', '节水', '补水', '饮用???', '水源', '水库', '水电', '雨污分离',\n                '湿地', '水体', '调水', '防洪', '输水', '河治???', '河综合治???',\n                '河改???', '水生???', '河综合整???', '水渠', '海水', '淡化', '流域综合管理',\n                '水资???', '调蓄', '水污???', '水务', '流域治理', '水质', '水塘', '雨污分流',\n                '净???', '废水', '水沟')\n  }\n  # better put fisrt level and 2nd level keyword to sift\n\n  keywords <- paste(keywords, collapse = '|')\n  index <- grep(keywords, array)\n  return(index)\n\n}\n\nwater1 <- siftWater(info1[, 'PROJ_NAME'])\nwater2 <- siftWater(info1[, 'PROJ_SURVEY'])\nwater <- union(water1, water2)\ninfo1_water <- info1[water,]\n\nwrite.table(info1_water, file = 'C:\\\\Users\\\\User\\\\Desktop\\\\1\\\\111.txt', fileEncoding = 'UTF-8', sep = '?')\n\n######################################################################################\n\nurl <- 'http://www.iea.org/statistics/statisticssearch/report/?year=2013&country=MYANMAR&product=Coal'\ntable <- url %>%\n  read_html() %>%\n  html_nodes(xpath = '//*[@id=\"stats-container\"]/div[2]/table') %>%\n  html_table()[[1]]\n\n\nres <- POST(url,\n            encode=\"form\",\n            body=list(country=\"JORDAN\",\n                      product=\"coal\",\n                      year = 2013), use_proxy(proxy[1, 1], proxy[1, 2]))\nresC <- content(res, as = 'text', encoding = 'utf-8')\n# But RJSONIO only gives list not dataframe\n# jsonlite will have encoding problems here, need to change sys encoding\n\ninfo <- jsonlite::fromJSON(resC, flatten = TRUE)\nreturn(info)\n\n\n\n###############################################################################\n# Machine Learning to categorize projects\n\n\n\n############################################################################\n# CSR website scraping\n# Since this is not a json string, but a javascript object, without quotes,\n# So package V8 is needed to parse\nlibrary(V8)\nlibrary(stringi)\n\nengine <- v8()\n\n\nurl <- 'http://stockdata.stock.hexun.com/zrbg/data/zrbList.aspx?'\ndate <- '2015-12-31'\npage <- 1\n\ndoCSRCorp <- function(startPage, endPage, proxy = NULL) {\n  page <- startPage\n  engine <- v8()\n\n\n  repeat {\n  res <- GET(url, query = list(date = date,\n                               count = 20,\n                               pname = 20,\n                               titType = 'null',\n                               page = page), use_proxy(proxy[1, 1], proxy[1, 2]), timeout(15))\n\n  resC <- content(res) %>%\n    stri_replace_first_fixed(\"hxbase_json1(\", \"var dat=\") %>%\n    stri_replace_last_fixed(\")\", \"\") %>%\n    engine$eval()\n\n\n  resC <- engine$get('dat')$list\n\n  if (page == 1) {\n    totalList <- resC\n  } else {\n    # bind the new list to the total list\n    totalList <- rbind(totalList, resC)\n  }\n\n  page <- page + 1\n\n  if (page > endPage) break\n\n}\n\n  return(totalList)\n}\n\nx <- doCSRCorp(1,151)\n\n\n\nresC <- content(res) %>%\n  stri_replace_first_fixed(\"hxbase_json1(\", \"var dat=\") %>%\n  stri_replace_last_fixed(\")\", \"\") %>%\n  engine$eval()\n\n\nresC <- engine$get('dat')\n\n# This failed, but put it here just in case\n# resC <- gsub('hxbase_json1', '', resC)\n# resC <- gsub('\\\\(\\\\{', '\\\\{', resC)\n# resC <- gsub('\\\\}\\\\)', '\\\\}', resC)\n# resC <- gsub('sum:', '\"sum\":', resC)\n# resC <- gsub('list', '\"list\"', resC)\n# resC <- gsub('Number', '\"Number\"', resC)\n# resC <- gsub('StockNameLink', '\"StockNameLink\"', resC)\n# resC <- gsub('industry', '\"industry\"', resC)\n# resC <- gsub('stockNumber', '\"stockNumber\"', resC)\n# resC <- gsub('industryrate', '\"industryrate\"', resC)\n# resC <- gsub('Pricelimit', '\"Pricelimit\"', resC)\n# resC <- gsub('lootingchips', '\"lootingchips\"', resC)\n# resC <- gsub('Scramble', '\"Scramble\"', resC)\n# resC <- gsub('rscramble', '\"rscramble\"', resC)\n# resC <- gsub('Strongstock', '\"Strongstock\"', resC)\n# resC <- gsub('Hstock', '\"Hstock\"', resC)\n# resC <- gsub('Wstock', '\"Wstock\"', resC)\n# resC <- gsub('target', '\"target\"', resC)\n# resC <- gsub('Tstock', '\"Tstock\"', resC)\n\n\nresC1 <- jsonlite::fromJSON(resC)\n\n\n############################################################################\n# Yellow river quality data\n\ngetWaterQ_MEP_all_unit <- function(year, week, station1, station2, proxy = NULL) {\n  res <- GET(url,\n             query = list(year = year,\n                          wissue = week), use_proxy(proxy[1, 1], proxy[1, 2]))\n  resC <- content(res, as = 'text', encoding = 'utf-8')%>%\n    read_html()%>%\n    html_table(fill = TRUE)\n\n  table <- resC[[1]]\n  startIndex <- which(table[,1] == station1)\n  endIndex <- which(table[,1] == station2)\n\n  table <- table[startIndex:endIndex, 1:14]\n  return(table)\n}\n\n\ngetWaterQ_MEP_all <- function(year, week, station1, station2){\n  message('Since the number of monitoring stations changes with time, so make sure in your\n        scraping period, the number of monitoring stations is consistent.')\n\n  url <- 'http://datacenter.mep.gov.cn/report/water/water.jsp?year=2016&wissue=45&x=29&y=6'\n  if (length(year) != 1) message('Caution!!! the result can be wrong if you input more than 1 year, since the number\n                                 of stations change with time.')\n  times <- 0\n  startTime <- Sys.time() # Get the start time, if it exceeds 1 hour, load proxy again.\n\n\n\n  # deal with proxy\n  proxyIndex <- 1\n  proxyPool <- getProxy()[,1:2]\n  page <- min(week)\n\n  repeat {\n\n    table <- tryCatch({\n\n      getWaterQ_MEP_all_unit(year = year, week = page, station1 = station1,\n                                  station2 = station2, proxy = proxyPool[proxyIndex, ])\n    },error = function(cond) {\n      message(paste('\\n', Sys.time(), \" Proxy doestn't work or ...\\n\"))\n      message(cond)\n      return(1)\n    })\n\n    if (length(table) == 1 & table == 1) {\n      times <- 0\n      proxyIndex <- proxyIndex + 1\n\n      if (proxyIndex == 300) { # there are 300 proxies at a time, if reaches 300, need to refresh\n        message('\\nrefreshe proxy pool...')\n        proxyPool <- getProxy()[,1:2]\n        proxyIndex <- 1\n      }\n\n    } else {\n\n      if (times == 0) {\n        totalTable <- table\n      } else {\n        # bind the new list to the total list\n        totalTable <- rbind(totalTable, table)\n      }\n\n      times <- times + 1\n      page <- page + 1\n      # here one more control, when page reaches 499, change proxy\n      # But if using proxy, usually no need to worry about the ip block\n      if (times == 1000000) {\n        # Just in case usually it will not reach this time\n        stop(\"1000000 times, change proxy...\\n\")\n        # randomNum <- round(runif(1, 1, 300))\n      } else {\n        message(paste(\"\\nweek\", page - 1))\n      }\n\n    }\n    if (page > max(week)) break\n    endTime <- Sys.time()\n    if (is.integer((endTime - startTime)/5400)) {\n      message(\"\\nRefresh proxy pool...\")\n      proxyPool <- getProxy()[,1:2]\n    }\n  }\n\n  return(totalTable)\n}\n\n\n\n\nSys.setlocale(category = \"LC_CTYPE\", local = \"Chinese\")\n\n\nwrite.table(a, file = 'C:\\\\Users\\\\User\\\\Desktop\\\\1\\\\111.txt')\n\n\n\n\n\na <- getWaterQ_MEP_all(2016,2:5, 1, 148)\n# for this website, there is no title, so it needs a little transition\ntable <- readHTMLTable(a, stringsAsFactors = FALSE)[[1]]\n\n\n############################################################################\n################# Stock tickers\nlibrary(httr)\nlibrary(rvest)\nlibrary(data.table)\n\nurl <- 'http://www.cninfo.com.cn/cninfo-new/fulltextSearch/full?searchkey=五矿发展有限公司&sdate=&edate=&isfulltext=false&sortName=nothing&sortType=desc&pageNum=1'\n\na <- GET(url)\nb <- content(a)\nticker <- b$announcements[[1]]$secCode\nname <- b$announcements[[1]]$secName\ndoubleCheck <- b$announcements[[1]]$announcementTitle\n\n\ngetTickers_unit <- function(corpName) {\n  url <- paste('http://www.cninfo.com.cn/cninfo-new/fulltextSearch/full?searchkey=', corpName, '&sdate=&edate=&isfulltext=false&sortName=nothing&sortType=desc&pageNum=1', sep = '')\n  a <- GET(url)\n  b <- content(a)\n\n  # check if something get back from the server\n  # and also has to check if there are at least 3 results, cus later on, it\n  # will pick up the 3rd result\n  if (length(b$announcements) == 0 | length(b$announcements) < 3) {\n    warnings (paste(corpName, 'could be a wrong name, recheck please'))\n    res <- data.frame(secName = 'wrong_name',secCode = 'wrong_name', doubleCheck = corpName)\n  } else {\n    # list number can be 1-10, but usually the 1st will be the full name,\n    # then comes the security name, so set 3, safer\n    secNameSplit <- unlist(strsplit(b$announcements[[3]]$secName, split = ','))\n\n    # if contains 债, probably a bond\n    index <- which(grepl(\"[[:digit:]]|债\", secNameSplit) == FALSE)\n    # check if only one index is back\n    if (length(index) != 1) warnings(paste(corpName, 'has more than two tickers, please double check.'))\n    secName <- secNameSplit[index]\n\n    secCode <- unlist(strsplit(b$announcements[[3]]$secCode, split = ','))[index]\n    doubleCheck <- b$announcements[[3]]$announcementTitle\n\n    if (length(index) == 0) {\n      warnings(paste(corpName, 'has no information in www.cninfo.com'))\n      res <- data.frame(secName = 'no_info',secCode = 'no_info', doubleCheck = corpName)\n    } else {\n      if (nchar(secName) != 4) warnings(paste(corpName, 'could be a wrong security name and wrong security ticker.'))\n      res <- data.frame(secName, secCode, doubleCheck)\n    }\n  }\n\n  return(res)\n}\n\n\n\n\ngetTickers <- function(corpNames) {\n  # here must be a column of company names\n  if (nrow(corpNames) == 1) {\n    res <- getTickers_unit(corpNames)\n  } else if (nrow(corpNames) > 1) {\n    # since listed companies are limited, no need to use data.table\n\n    for (i in 1:nrow(corpNames)) {\n      res1 <- getTickers_unit(corpNames[i,])\n      if (i == 1) {\n        res <- res1\n      } else {\n        res <- rbindlist(list(res, res1))\n      }\n      message(i)\n    }\n  } else {\n    break(\"Please input full name(s) of a company or a column of companies, must be a column, not a row\")\n  }\n\n  return(res)\n\n}\n\n\n########### get market cap\n\ngetMktCap_unit <- function(ticker, date1, date2) {\n\n\n  url1 <- 'http://quotes.money.163.com/service/chddata.html?'\n  url2 <- paste('code=', '0', ticker[[1]], sep = '')\n  url3 <- paste('&start=', date1, '&end=', date2, sep = '')\n  url4 <- '&fields=TCAP;MCAP'\n\n  url <- paste(url1, url2, url3, url4, sep = '')\n\n  res <- read.csv(url)\n  if (nrow(res) == 0) {\n    newRes <- data.frame('no_data', as.character(ticker), 'no_data', 'no_data', 'no_data')\n    names(newRes) <- names(res)\n    res <- newRes\n  }\n  return(res)\n}\n\n\ngetMktCap <- function(tickers, date1, date2) {\n  tickers <- tickers[[1]]\n  if (length(tickers) == 1) {\n    res <- getMktCap_unit(tickers, date1, date2)\n  } else if (length(tickers) > 1) {\n\n    # if the result comes from getTickers, it's factor, need to be converted\n    # if (is.factor(tickers) == TRUE) {\n    #   tickers <- as.character(levels(tickers))[tickers]\n    # }\n    #\n\n    for (i in 1:length(tickers)) {\n      res1 <- getMktCap_unit(tickers[i], date1, date2)\n      if (i == 1) {\n        res <- res1\n      } else {\n        res <- rbindlist(list(res, res1))\n      }\n      message(i)\n    }\n\n  } else {\n    break('Please input a column of tickers, column, not row')\n  }\n  return(res)\n}\n\n\n\n\ngetIndex <- function(indexPool) {\n  for (i in 1:nrow(indexPool)) {\n    url <- as.character(indexPool[i, 2])\n    # index information comes from 2 different source by now, so it needs to\n    # detect wether it's from sina or etnet\n    if (grepl('sina.com', url)) {\n      listNum <- 4\n    } else if (grepl('etnet.com', url)){\n      listNum <- 1\n    } else break ('Wrong url in indexPool')\n\n    data <- html_table(read_html(url), fill = TRUE)[[listNum]][, 1:2]\n    data <- data[2:nrow(data), ]\n    data[,3] <- rep(indexPool[i,1], nrow(data))\n    colnames(data) <- c('ticker', 'name', 'index')\n    rownames(data) <- NULL\n\n    if (i == 1) {\n      tData <- data\n    } else {\n      tData <- rbindlist(list(tData, data))\n    }\n  }\n\n  return(tData)\n}\n\n\nindexConstnt <-function(tickers) {\n\n  #creat an index pool\n  indexPool <- data.frame(index = c('CSI_100', 'SSE_50', 'CSI_300', 'SSE_Central_SOEs_50',\n                                    'HSI', 'HSCEI'),\n                          url = c('http://vip.stock.finance.sina.com.cn/corp/go.php/vII_NewestComponent/indexid/000903.phtml',\n                                      'http://vip.stock.finance.sina.com.cn/corp/go.php/vII_NewestComponent/indexid/000016.phtml',\n                                      'http://vip.stock.finance.sina.com.cn/corp/go.php/vII_NewestComponent/indexid/000300.phtml',\n                                      'http://vip.stock.finance.sina.com.cn/corp/go.php/vII_NewestComponent/indexid/000042.phtml',\n                                      'https://www.etnet.com.hk/www/tc/stocks/indexes_detail.php?subtype=HSI',\n                                      'https://www.etnet.com.hk/www/tc/stocks/indexes_detail.php?subtype=CEI'\n                                      )\n                            )\n\n  data <- getIndexConstnt(indexPool)\n\n  indexTable <- data.table(tickers)\n\n  emptyCols <- data.frame(matrix(0, nrow = nrow(tickers), ncol =nrow(indexPool)))\n\n  indexTable[, c(as.character(indexPool$index)) := emptyCols]\n\n  for (i in tickers) {\n\n  }\n\n}\n\n\ngetExchange_unit <- function(stockName) {\n  engine <- v8()\n  url1 <- 'http://so.hexun.com/ajax.do?key='\n  url2 <- '&type=all?math=0.6109926008061326'\n  url <- paste(url1, stockName, url2, sep = '')\n\n#  a <- content(GET(url))\n\n  res <- content(GET(url), as = 'text') %>%\n    stri_replace_first_fixed(\"hxSuggest_JsonData=\", \"\\\"list\\\":\")\n\n  res1 <- paste('{', res, '}', sep = '')\n\n  resC <- fromJSON(res1)\n\n\n}\n\n\n\n\n\na <- POST(url, encode=\"form\",\n            body=list(key = 600601,\n                      #orgId = 9900013088,\n                      type = 'stock'\n                      ))\nb <- content(a, as = 'text', encoding = 'utf-8')\n\n\n",
    "created" : 1488731569525.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3809134587",
    "id" : "F3530072",
    "lastKnownWriteTime" : 1489073595,
    "last_content_update" : 1489073595230,
    "path" : "C:/Users/user/Google Drive/CWR/R/scraping.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}